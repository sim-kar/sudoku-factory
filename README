## About
This is an application for playing [Sudoku](https://en.wikipedia.org/wiki/Sudoku). It can 
generate random Sudoku puzzles of varying difficulty for you to try to solve. The puzzles are 
interactive, and give you feedback about your progress.

### How to run
1. From the terminal/command line, navigate to the folder containing the project. 
2. Run the command `mvn package` and wait for it to finish.
3. Run the command `java -jar target/sudoku-1.0-SNAPSHOT.jar`.

### How to use
When the application first starts, there is no Sudoku puzzle. A new puzzle can be generated by 
selecting a difficulty from the right-hand panel and clicking the *New Puzzle* button. The 
selected difficulty will dictate how many clues—fixed digits with the correct value—are on the 
generated puzzle. It is possible to generate as many new puzzles as you would like.

![Started application](src/main/resources/app_empty.png)

It is possible to interact with a generated puzzle by clicking on an empty tile to select it, 
and using the keyboard to set the value. The keys *1-9* set the corresponding value, while the 
keys *0*, *delete* and *backspace* clear the tile.

![A generated puzzle](src/main/resources/app_puzzle.png)

The currently selected tile is marked with a light blue background. To differentiate digits 
entered by the user (that are editable) from clues (that are not editable), they are colored 
blue and black, respectively. If *Show Duplicates* is enabled in the right-hand panel, entered 
digits with a duplicate in a row, column or block are colored red. This can be disabled.

![A selected tile and digits entered by the user](src/main/resources/app_input.png)

Another option that can be enabled is *Show Hints*. This will highlight all sections that 
contain incorrect digits added by the user red.

![A puzzle with hints being shown](src/main/resources/app_hints.png)

When the entire puzzle is solved, the background will be colored green to reflect this.

![A solved puzzle](src/main/resources/app_solved.png)

## Environment & Tools
| Tool               | Version                                          |
|--------------------|--------------------------------------------------|
| OS                 | Windows 10 19044.1706 & Linux Mint 20.2 Cinnamon |
| IDE                | IntelliJ IDEA 2021.3.3 (Ultimate Edition)        |
| Java version       | OpenJDK 16                                       |
| Maven version      | 3.6.3 & 3.8.5                                    |
| JUnit version      | 5.8.2                                            |
| Mockito version    | 4.5.1                                            |
| Git version        | 2.29.2.windows.1 & 2.36.1                        |
| Repository hosting | BitBucket                                        |

## Purpose
### The Application
The goal is to create an application that can generate random Sudoku puzzles, and present them 
in a GUI that the user can interact with to try to solve them. The puzzles must conform to the 
rules of [Sudoku](https://en.wikipedia.org/wiki/Sudoku), which includes the rule that there can 
only be one valid solution to a puzzle.

It should be possible to select the difficulty of generated puzzles. Concretely, the difficulty 
will be determined by the number of clues on the board, with fewer clues being more difficult.

The user should also be able to interact with the puzzle by selecting tiles on the board that 
aren’t clues, and changing their value. According to the rules of Sudoku, the possible values 
for a tile are 1-9 or empty.

The GUI should present the current state of the game to the user, and present all options 
available to the user, such as setting the difficulty and generating a new puzzle. It should 
also give visual feedback when a puzzle is solved, and be able to highlight duplicate tiles in 
rows, columns or blocks.

### The Workflow
Another important aspect is to create a workflow to use for the project. This includes:

- the different kinds of tasks that contributors can work on;
- the git workflow, dictating how to use branches for individual tasks and how to do pull requests;
- how to conduct peer reviews of completed tasks;
- conventions for code and documentation; and
- how to track tasks and progress.

### TDD
The final goal is to implement the project using Test-Driven Development. Unit tests that 
express the desired outcome should be created before the production code is written. The 
production code should then be written to make the tests pass. The idea is that the tests should 
inform the behavior of the implementation, not vice-versa.

## Procedures
### Creating the Workflow
The work that was to be completed in the project was broken down into *tasks.* Five categories 
of tasks were defined: feature, patch, test, bug and technical. These inform what kinds of 
activities a single task contains. See **Task Guidelines** in the appendix for a more in-depth 
description of each task.

To track the progress of tasks, a kanban board was created. The board was given four columns to 
track the different stages of progress:

1. **Backlog**: a list of all tasks that need to be completed, in order of priority.
2. **In Progress**: tasks currently being worked on by a contributor. These had a limit of one 
   task per person.
3. **Review**: tasks that a contributor has finished, awaiting review by another contributor 
   before they can be deemed completed.
4. **Completed**: tasks that have passed review and can be assimilated into the final product.

A git workflow was developed to work in conjunction with this. When a task is taken from the 
backlog, the contributor creates a new feature branch dedicated to that task. The branch has a 
naming convention that ties it to the task, and any work relating to the task is committed to 
this branch. When finished with the task, the contributor creates a pull request of the feature 
branch to the master branch. At this stage, another contributor must review the work before it 
can be accepted and finally merged into the master branch. A full description of the workflow 
can be found in the appendix under **Git Workflow.**

A review process was also created, that dictates that another contributor must review a finished 
task before it is accepted. This entails checking that the work fulfills the requirements 
outlined in the task, and that it conforms to the defined code conventions and documentation 
guidelines. Additionally, upon review requirements that were not explicitly stated in the task 
might become apparent. In such cases, it was decided that the task should be rejected and the 
new requirements added, rather than having to address possible issues further down the line. See 
**Peer Review Guidelines** in the appendix for the full description.

Finally, code conventions and documentation guidelines were created. The Code Conventions 
contains generalizations and principles aimed at writing higher quality code, as well as code 
style guidelines for writing more readable code. The full text can be found under **Code 
Conventions in** the appendix. The **Documentation Guidelines** were aimed at how to write 
helpful documentation that serves a purpose rather than only being added as a formality. One 
notable detail is that for unit tests, using a descriptive `@DisplayName` that details what is 
being tested, and what the expected outcome is, was chosen as the default documentation method 
over *javadoc*—although the latter was also permissible as an addition when more details are 
required. See **Documentation Guidelines** in the appendix for more details.

### Working with TDD
TDD was also an instrumental part of how the project was carried out. With an emphasis on 
writing unit tests, the code needed to be easy to test. Although there were several guidelines 
to this end in the code conventions, two in particular were used heavily to guide the overall 
architecture and class design: the single responsibility principle and avoiding side 
effects—especially having classes avoid creating their own dependencies.

![A simplified UML class diagram of the application](src/main/resources/sudoku_uml.png)

*Fig 1. A simplified UML class diagram of the application.*

The overall architecture shown in Fig 1. illustrates several examples of these guidelines in 
action, most noticeably how not creating your own dependencies is used. For instance, *Solver* 
uses a [`Random`](https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/util/Random.html) 
that is passed as a parameter, instead of creating one on its own or using something like 
`Math.random()`. Testing random output is generally something to be avoided, but doing it this 
way we were able to control the output by passing a `Random` with a predetermined seed to the 
solver. This makes testing easy since we can control the inner state of the solver. The same 
principle is used throughout the project to produce more easily testable code.

The general design methodology was to first come up with the overall architecture by dividing 
the project into smaller and smaller individual components. The goal was to end up with cohesive 
classes that  had a single responsibility. In order to write unit tests for a class, a public 
interface needed to be devised. To do so, we considered the behavior that we wanted the class to 
exhibit, and what tests were needed to test that behavior. In turn, that dictated the design of 
the public facing interface.

With that in place, the TDD approach *red, green, refactor* could be used. It was an iterative 
process where tests were written for the desired behavior first, and run to make sure they failed.
This sometimes posed a problem. In order to run the tests, the method being tested had to first 
be created, even if only as an empty “skeleton” to be implemented later. If the method returns a 
primitive, it cannot return something other than that primitive type. In the case of a `boolean`,
the method must return either a `true` or a `false`, or it will generate an error and cannot be 
run. When writing unit tests to check whether a board is correct, we wanted one test to see that 
it correctly returns `true` if all tiles on the board are correct, and one to see that it 
correctly returns `false` if all tiles are not correct. This posed a dilemma, since the method 
had to return either `true` or `false`, one test would always be successful. We found no way 
around this, other than changing the return value of the method and running the tests again to 
see the opposite test fail. That way, at least the tests had both failed, albeit not at the same 
time.

In the beginning, we also ran into some other problems related to writing the tests first. If a 
task was to implement a single method in a class that implemented an interface for example, all 
methods in that interface had to be implemented (again, as empty “skeletons”) in order to run 
any tests. This could be solved by first making a task for implementing all empty methods, but 
this of course led to more overhead. Overall, finding an appropriate scope for a single task was 
not always evident, especially at first.

The tests themselves were written to focus on behaviors, not the underlying details. If a public 
method relied on a helper method, they were not tested separately. By testing the public 
interface, the underlying details were also implicitly tested.

After the tests were in place, the code to make the tests pass was written. The focus was on 
creating a quick solution, not necessarily a good one. The emphasis here was on understanding 
the solution, so that it could be improved in the next step.

Once the tests passed, another pass of the code was made to improve its quality, without 
changing its behavior. Here, principles laid out in the code conventions were applied, such as 
**Do not repeat yourself**, the **Single responsibility principle**, and **be explicit** (these 
can be seen in more detail in the **Code Conventions** in the appendix). This step was not 
always the end of the process. Rather, it was an iterative process, that sometimes involved 
going back to refactor or even add new tests as well.

### Creating the Application
Fig 1. gives a simplified overview of the classes in the program. For simplicity, we will not 
differentiate between abstract and concrete classes.

The program was created using a bottom-up approach, with the smaller components that were used 
by the bigger components being created first. This meant that *Position* was created first, 
followed by *Tile*, *Section*, *Board* and so on. Together, these represent the different parts 
of a complete Sudoku puzzle and its solution.

To create new puzzles, we created a *Factory*, which is responsible for making sure that the 
created puzzles conform to the rules of Sudoku. To this end, it was crucial to first create a 
*Solver* that could solve a given Sudoku puzzle. The general idea is that giving the solver an 
empty board to solve would generate a solution for that board, which could then be used to 
create a new puzzle by removing digits from it.

To solve a puzzle, a backtracking algorithm was implemented. It tries to come up with a solution 
recursively by adding a digit to all empty tiles in order and seeing if the solution is correct. 
If the solution isn’t correct, it tries adding the next digit to the current—in other words, the 
last empty—tile. If none of the digits result in a correct solution, it backtracks to the 
previous tile and tries the next digit there. This is repeated until either a solution is found, 
or until every possible combination of digits has been tried.

If the digits used to come up with a solution are in a random order, this can also be used to 
create a randomized Sudoku board. But for a Sudoku board to be a puzzle, it needs to have empty 
tiles for the user to fill in. One extra consideration that had to be taken at this point was 
that a generated puzzle could only have a single unique solution in order to be a valid Sudoku 
puzzle. To achieve this, the same backtracking algorithm was used again, but with the digits 
being tested in a different order. One digit was removed at a time from the board, and the 
solver was used to solve the puzzle with digits in both ascending and descending order. Since 
the two solutions would prioritize using digits in the opposite order, if they were the same, it 
meant there was only a single possible solution. In this way, the same method could be used to 
both generate new fully-solved Sudoku boards, and to generate valid puzzles from those boards, 
by simply changing the order in which digits were being tested.

Finally, it is all put together in the GUI, which was implemented using the 
Model-View-Controller, or MVC pattern. The *Model* tracks the state of the game. It uses a 
factory to create a board, and provides an interface to try to solve it. Since creating a new 
board is a computationally expensive operation, it is performed in a background thread by an 
anonymous 
[`SwingWorker`](https://docs.oracle.com/en/java/javase/16/docs/api/java.desktop/javax/swing/SwingWorker.html). 
That way it doesn't interfere with the responsiveness of the GUI. This proved challenging to 
unit test, since the main thread doesn't wait for the result from the background thread before 
continuing. To solve this, the method under test was given a 
[`CountDownLatch`](https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/util/concurrent/CountDownLatch.html) 
as an optional parameter, that could be used to wait for the `SwingWorker` to be done. In 
addition, all associated tests were given a `@Timeout` so that they wouldn't keep the rest of 
the tests waiting forever if the `SwingWorker` never finished (such as when the code hadn't been 
written yet).

To get changes from the model, a pull system using the Observer pattern was created. Observers 
could register with the model, and were notified by the model when its state was updated, such 
as when a new puzzle was created. The observers could then pull the changes directly from the 
model themselves.

The *View* was implemented as an observer of the model. It is responsible for displaying the 
state of the model and providing a graphical interface for the user to interact with. It doesn't 
handle any of the actual game logic—that is done by the model. Also, the view doesn't update the 
model directly. As you may recall, puzzle difficulty is dictated by the number of clues on the 
board. This is an implementation detail that we did not want the view to concern itself with. 
Instead, we used the *Controller* to provide the strategy for the view—it translates requests 
from the view to the model so that the view doesn't have to know details about the inner 
workings of the model. In essence, the view is registered as an observer of the model, and uses 
the controller to update it. When it has updated, the model notifies its observers—in this case 
the view—who can then pull the changes directly from the model.

Writing unit tests for the GUI was a bit more challenging than the rest of the program, since it 
often involved making requests to other components with no easy way of verifying the result. 
However, this could for the most part be solved with mocking. With the help of the mocking 
library Mockito, we were able to test almost all behavior in the model and controller. But the 
view mostly involved creating and drawing various components, and receiving and reacting to user 
input, which was not suited to testing using unit tests. There were some private helper methods 
that we were able to test without having to break encapsulation by using 
[reflection](https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/reflect/package-summary.html). 
But by and large, we had to settle on testing most of it manually by running the application and 
interacting with it, which to a large degree precluded using TDD to implement the view.

## Discussion
A major cornerstone of this project was collaboration and the actual process that was used to 
complete the work. To this end, a workflow was created and tracked using a shared kanban board. 
The workflow itself, such as the different types of tasks, how to manage branching in git and 
how to perform peer reviews was largely successful. While it did mean more overhead and time 
spent performing “administrative” tasks, it also helped provide organization and documentation 
of the work being performed, and gave an overview of the overall progress. Moreover, the review 
process meant that both contributors were more familiar with the entire code base, and helped 
uncover issues that might otherwise have gone unnoticed. This led to higher quality code.

One challenge with the workflow however was setting the scope of individual tasks. We initially 
wanted the tasks to be small. The reasoning was that most components relied on the 
implementation of another component, and if that implementation was a single task it meant that 
one contributor would be forced to wait for it to be finished before being able to proceed with 
the next task. If there were instead many smaller tasks, contributors could work more in 
parallel. But in turn, this meant more overhead in the form of git and kanban board management, 
peer reviews and so on. To allow work to be done in parallel, the classes being worked on also 
had to be implemented beforehand, with a “skeleton” interface of the public methods in order for 
unit tests (and the methods in the class being worked on) to be executable. This added yet 
another task that had to be done and reviewed before work could proceed. As the project 
progressed, tasks increased in scope. Both to avoid some of these issues, but also since it was 
simpler to implement certain components in their entirety.

Working with TDD was also a learning process that didn't necessarily feel natural at first. This 
was exacerbated by the small scope and simplicity of tasks in the beginning. One frustration was 
the *red* step in *red, green, refactor*. As we mentioned in **Procedures**, it sometimes wasn't 
possible to make two tests fail at the same time, such as when testing a method with a `boolean` 
return value. Perhaps being a bit pragmatic in such cases, and instead testing both return 
values (testing for both `true` and `false` output using `assertAll`) in the same unit test 
would work as an option. However, this would require bending the rule of a unit test having a 
single reason for failure.

Leaving aside such small gripes, TDD brought two substantial benefits to the project:

1. It forced us to think more carefully about the behavior of the program before implementation 
   began, which informed both input and output, and in turn the public interface design as a 
   whole. This possibly led to a more thought-out design overall.
2. It provided a more readily available feedback mechanism for verifying the correctness of the 
   implementation. Being able to quickly and easily perform automated tests of the 
   implementation was a godsend. We assume that this led to a more coherent code base with fewer 
   subtle bugs, and less time spent debugging.

The downside however is that a lot of time has to be spent upfront to design and write the tests.
But if those tests are written later on instead that would only mean postponing the time being 
invested, without any clear benefits.

For unit tests in general, we focused on testing behavior instead of implementation details. 
This meant not testing private helper methods in isolation. The reasoning was that such methods 
may change when refactoring, which is an essential step in TDD. By explicitly testing such 
details, they would become coupled to the tests, and it could lead to having to either redo the 
tests during refactoring, or being unable to change those details altogether. This of course 
could lead to situations where we would be hesitant to refactor parts of the program. By 
focusing our tests on the desired behavior, these implementation details were implicitly tested.
If the public interface exhibits the desired behavior, you can deduce that the private helper 
methods that it uses also exhibits the desired behavior (or at least that the combination of the 
two do so). An exception to this was private helper methods in the view. The view was not very 
suited to unit tests, since most behavior was triggered by user interaction in the GUI. 
Therefore, we were unable to unit test these methods implicitly by testing the public interface.
Instead, we used reflection to access the private methods and test them individually. To 
alleviate any issues with refactoring, the tests for each method were isolated in their own 
nested classes to make it easier to remove or otherwise handle them if necessary down the line.

When it came to documenting unit tests, the reasoning behind using the `@DisplayName` annotation 
as the main method of documentation was that unit tests should be small and cohesive, only 
testing a single thing per test. If it was difficult to document in a single sentence, it was a 
sign that the test was too complex, and needed to be broken down into several smaller tests or 
reconsidered. This reasoning seems to have held up well, judging from the scope of tests. 
However, one thing that we did not foresee was the size and complexity of the input for certain 
tests. A Sudoku board comprises many interconnected parts—digits, tiles, rows, columns and 
blocks that have to be taken into account. This of course was reflected in the input. While the 
documentation guidelines stated that it was possible to use javadoc to document the input such 
cases, that suggestion was not enough. When implementing the tests, we would try to choose as 
simple and understandable input as possible, that seemed perfectly clear at the time. But upon 
later review, it was not as easy to understand. Some additional documentation was added after 
the fact, but it is clear that more strict guidelines or rules regarding how and when to 
document test input data would have been preferable, instead of leaving it entirely to the 
implementer's judgment.

The final application contains all the primary features that were set as goals in the beginning 
of the project. It provides a GUI where the user can generate new, interactive Sudoku puzzles of 
varying difficulty, and try to solve them. The individual components that make up the 
application were designed to be easy to test, and to have a clear separation of concerns. A 
crucial piece of the application was the solver, which is used to create the Sudoku puzzles. It 
was important that the puzzles conformed to the rules of Sudoku, including only having a single 
unique solution. We were able to achieve this by using a recursive backtracking algorithm that 
served the dual purpose of both generating a puzzle, and being able to check its uniqueness. 
This backtracking algorithm is not the fastest, which did limit the lowest amount of clues 
available to us when generating new puzzles—if we wanted the GUI to still feel responsive. But 
since the solver is injected, and implemented as an interface in the factory, it can easily be 
swapped for another solver that implements a faster algorithm down the line. The entire project 
is designed to be easily extensible in this way.

# Appendix
# Task Guidelines
Tasks represent a single activity that needs to be completed. There are several types of 
activities, denoted by one or more of the following labels:

- **FEATURE** = Green: a feature to be implemented.
- **PATCH** = Light Blue: a modification to the behavior of an already implemented task.
- **TEST** = Yellow: a test case to be implemented.
- **BUG** = Red: a problem that needs to be fixed.
- **TECHNICAL** = Dark Blue: changes that do not alter behavior or implementation details, such 
  as documentation or Git management.

A task can have more than one label. For example, labels can be added when a task is peer 
reviewed as per [https://trello.com/c/d4WBhjxR/4-peer-review-guidelines](https://trello.com/c/d4WBhjxR/4-peer-review-guidelines). 
For each label on a task card, a corresponding header is added with a description.

## FEATURE
Means the implementation of new behavior. The descriptions should specify *what* to implement, 
but can also describe *how* (implementation details) and *why*.

## TEST
Even though all implemented behavior should include tests, it is not mandatory to add a 
**TEST** tag to a task. Tests will be added when the task is implemented either way, and it is 
not efficient, or always possible even, to exhaustively come up with all test cases in advance. 
Use the **TEST** label when you want to document some necessary test cases in advance, or find 
a test case that is missing during peer review.

## BUG
When something isn't working as intended, or causes unintentional side effects or other 
problems. Should include a report of the current behavior, and what it should be doing instead.

## PATCH
When you want to change intended behavior. In other words, it is not a bug fix but a change 
from the current implementation.

## TECHNICAL
For changes that do not concern the program's behavior. This can be things such as 
documentation or Git management.

---

# Git Workflow

![Git Workflow](src/main/resources/git_workflow.png)

- When starting work on a task, move it from the **Backlog** to **In Progress**, and set 
  yourself as owner of the card (*Right-hand menu > Members*).
Create a new git branch with the naming convention `task/name-in-kebab-case`, where task is the 
  task type (feature, bug, patch), and the name is the name of the task card. If a task has 
  several labels, use the most appropriate label. Typical priority is Feature > Bug > Patch > 
  Test > Technical. Example: `feature/create-game-board`
- When working on the task, commit all work to this new branch. Try to favor frequent, small 
  commits. This makes it easier to go back if future changes introduce problems.
- When the task is finished, make a pull request to the master branch, and move the task card 
  to the **Review** column. Change the owner of the card to the member who will perform the 
  review (*Right-hand menu > Members*), and add a link to the branch on Bitbucket at the top of 
  the card.
- Another contributor must peer review the task, according to the 
- [https://trello.  com/c/d4WBhjxR/4-peer-review-guidelines](https://trello.com/c/d4WBhjxR/4-peer-review-guidelines). 
- If the task passes the review, it is tagged with the **ACCEPTED** label. Otherwise, it is 
  marked with the **DECLINED** label and moved to the top of **Backlog**.
- When your task is marked with **ACCEPTED**, you can merge it to the master branch, and move 
  the task card to the **Completed** column.

---

# Peer Review Guideline
Before a task is deemed to be finished, and can be moved to the *Completed* column, it must 
pass a peer review. To mark a task for review, move it to the **Review** column, and change the 
owner of the card to the member who will perform the review (*Right-hand menu > Members*). Also 
make sure to include a link to the branch on Bitbucket at the top of the card!

To pass a peer review, a task must conform to the requirements laid out by the task card, and 
conform to the 
[https://trello.com/c/UhPnKUUZ/5-code-conventions](https://trello.com/c/UhPnKUUZ/5-code-conventions) 
and [https://trello.com/c/wLGgpb4x/6-documentation-guidelines](https://trello.com/c/wLGgpb4x/6-documentation-guidelines).

It is also possible for the reviewer to uncover requirements that were not stated in the 
original task, but which may cause issues if not addressed. In such a case, tag the task with 
the **PATCH** label and add the new requirements under the **PATCH** header in the description.

Similarly, if there is an issue with the implementation, add the **BUG** tag and add relevant 
information under the **BUG** header in the descriptions. The same approach can be applied with 
**TEST** and **TECHNICAL** as well.

If a task fails the review, the reviewer tags the task with the label **DECLINED** puts it at 
the top of the **Backlog** column for it's owner to pick back up. If it is approved, the 
reviewer tags it with the **ACCEPTED** label. Once the owner has merged the task's branch into 
the master branch, he/she can move it to the **Completed** column.

---

# Code Conventions
**Be explicit:** Try to not rely on context, or implicit understanding. Don’t favor making code 
shorter or “more efficient” at the cost of being clear and explicit.

**Single responsibility principle:** A method or class should have a single responsibility. If 
you find that it is doing several different things, consider refactoring into several smaller 
methods or classes. This will increase cohesion and lower coupling. It also leads to more 
easily testable code.

**Do not repeat yourself (DRY)**: If you find that you are repeating the same or similar code 
in multiple places, consider breaking the code out into its own function.

**Fail early, fail hard:** If the program runs into an exception, it is often better to let the 
exception be thrown (add the `throws` clause to the method) than catching and suppressing it 
early. Catching the error can hide it, making it harder to figure out problems down the line.

Also, always strive to catch and throw appropriate exceptions; remember to be explicit and 
don't just use the most general type `exception`.

**Writing testable code:** Avoid side effects with your code. One example is classes and 
methods that create their own dependencies, i.e. they create the things they need themselves. 
Instead, consider passing this dependency as a parameter instead - ideally as an interface. It 
will make the code more easy to test.

**Test one thing**: A unit test should test a single behavior, and should not affect, or be 
affected by, other tests.

**Writing unit tests:** The [AAA](http://wiki.c2.com/?ArrangeActAssert) pattern can be used to 
format a unit test:

1. **Arrange** all necessary preconditions and inputs.
2. **Act** on the object or method under test.
3. **Assert** that the expected results have occurred.

If it is hard to write a unit test, it's an indication that there might be a problem with the 
code being tested. In such cases, consider refactoring the code.

Try to avoid testing specific implementation details. Instead focus on testing the behavior 
that you want (or don't want) the code to exhibit. Too much emphasis on testing the details of 
an implementation can lead to tight coupling between unit tests and implementation details, 
which means the code can't be refactored without also changing the tests. This can lead to 
developers not refactoring the code out of a fear of breaking the tests.

**Input for unit tests:** Avoid random functions, loops, and conditionals in unit tests. If you 
want to test the state after adding items, `0`, `1` and `2` items is enough. Focus on testing 
edge cases. That is, values that are *just* inside and outside the allowable range of values. 
That is where most errors tend to happen. Also test `null` inputs where appropriate.

## Naming
**Classes and interfaces:** `CamelCase`.

**Packages:** lowercase without separation between words, e.g. `com.somepackage`.

**Methods:** `pascalCase`. Method names should be verbs, e.g. `calculateResult`. However, 
adjectives may suit methods that return booleans better, e.g. `isEmpty`, `greaterThan`.

**Constants:** uppercase with `_` separating words, e.g. `CONSTANT_VALUE`.

**Variables:** use descriptive names. Try to avoid just a single letter, except where it is 
explicit and obvious, such as in for-loops. Avoid long names with metadata (*number*, *map*, 
*set*, etc.). A variable name should easily tell the reader what it represents:

```java
//Avoid (❌) - Too detailed variable naming
int schoolIdentificationNumber;
int[] userProvidedSchoolIds;
int[] schoolIdsAfterRemovingDuplicates;
Map<Integer, User> idToUserMap;
String valueString;

// Prefer (✔️) - Variable names short and descriptive
int schoolId;
int[] filteredSchoolIds;
int[] uniqueSchooldIds;
Map<Integer, User> usersById;

// Prefer (✔️) - Include unit in variable name or type
long pollIntervalMs;
int fileSizeGb;
Amount<Integer, Data> fileSize;

```

In general, avoid acronyms and abbreviations. However, acronyms that are widely known and used, 
such as HTTP and XML can be used. Such acronyms should be treated as regular words and not be 
fully capitalized, e.g. `XmlReader` for classes, or `sendHttpRequest` for methods.

**Unit tests**: Name testing classes after the class they are testing: `ClassName` becomes 
`ClassNameTest`.

Each test should have a descriptive name that makes it clear what is being tested. Adding 
`test` to the name is unnecessary since it already has the `@test` annotation. Use descriptive 
names that make it clear what is being tested.

```java
//Avoid (❌)
@Test
public void testAddingTwoNumbers()

// Better (✔️)
@Test
public void addingTwoNumbers()

// Best (✔️) - include expected behavior
@Test
public void addingTwoNumbersReturnsCorrectAnswer()

```

## Code Style
**Indentation:** 4 spaces. Line wraps should be (at least) double the indentation, 8 spaces.

**Line length:** 100 gives a “*balance between fewer continuation lines but still easily 
fitting two editor tabs side-by-side on a reasonably-high resolution display.*”

**Spacing:** Use spaces between operators, and before opening braces:

```java
// Avoid (❌)
int foo=a+b+1;

// Prefer (✔️)
int foo = a + b + 1;

// Avoid (❌)
if(a==b){
    statement;
}

// Prefer (✔️)
if (a == b) {
    statement;
}

```

**Line breaks:** Avoid the temptation of saving 1–2 lines of code at the expense of readability.
Can be used anywhere it improves readability, or you want to logically separate code. For 
example, adding a blank line before a line comment and after the code it comments on. Always 
add 1 blank line between methods, and other blocks that end with `}`.

```java
// Avoid (❌) - No separation between methods
public void method() {
    ...
}
public void anotherMethod() {

// Prefer (✔️)
public void method() {
    ...
}

public void anotherMethod() {

// Avoid (❌) - No logical separation
someMethod();
// comment explaning the next two lines
int newVariable();
doSomething(newVariable);
someOtherMethod();

// Prefer (✔️) - Separate commented code with blank lines
someMethod();

// comment explaning the next two lines
int newVariable();
doSomething(newVariable);

someOtherMethod();

```

**Methods:** Prefer parameters on their own lines if there are many:

```java
// Prefer (✔️) - Easy scanning and extra column space.
public String downloadAnInternet(
        Internet internet,
        Tubes tubes,
        Blogosphere blogs,
        Amount<Long, Data> bandwidth
) {
    tubes.download(internet);
    ...
}

```

**If-statements:** do not omit braces on multi-line if statements. Short, one-line statements 
can omit bracers. When using line wrapping with several statements, put `&&` or `||` on the new 
line. Also when line wrapping, make sure the indentation of conditionals isn’t the same as the body.

```java
// Avoid (❌) - Do not omit {} on multi-line statements
if (condition)
  statement;

// Prefer (✔️)
if (condition) {
  statements;
} else if (condition) {
  statements;
} else if (condition) {
  statements;
}

// Prefer (✔️) - Can skip {} on short one line statements
if (x < 0) return true;

// Avoid (❌)
if ((condition1 && condition2)
    || (condition3 && condition4)
    ||!(condition5 && condition6)) { // BAD WRAPS
    doSomethingAboutIt(); // MAKE THIS LINE EASY TO MISS
}

// Prefer (✔️)
if ((condition1 && condition2)
        || (condition3 && condition4)
        ||!(condition5 && condition6)) {
    doSomethingAboutIt();
}

```

**Multi-line mathematical expressions and String concatenation:** Put operators on the new line 
with the value:

```java
// Avoid (❌) - Operator on previous line
int sum = value1 +
          value2 +
	      value3;

// Prefer (✔️) Operator on same line as value
String concatenatedString = string
                            + "some additional text"
                            + string2;

```

**Operator precedence:** Be explicit by using parenthesis to indicate operator precedence:

```java
// Avoid (❌)
return a * 8 * n + 1 / 4;

// Prefer (✔️)
return (a * (8 * n) + 1) / 4;

```

**Method chaining:** If there are several method calls, put them on their own line:

```java
// Avoid (❌)
int result = someMethod(value).doSomething(anotherValue)
    .process().finish();

// Prefer (✔️)
int result = someMethod(value)
    .doSomething(anotherValue)
    .process()
    .finish();

```

**Favor readability:** If there is an ambiguous and unambiguous choice—always go with the 
unambiguous:

```java
// Avoid (❌) - it may be difficult to tell 1 and l apart
long count = 100l + n;

// Prefer (✔️)
long count = 100L + n;

```

**Imports:** Avoid wildcard imports, e.g. `import java.util.*;`. If using IntelliJ, this can be 
changed with the settings: *IntelliJ > Settings > Editor > Code Style > Java > Imports > Change 
“Class count to use import with ‘\’” and “Names count to use static import with ‘\’”.*

Some examples are taken from 
[A short summary of Java coding best practices](https://rhamedy.medium.com/a-short-summary-of-java-coding-best-practices-31283d0167d3), 
and [Twitter’s Java Style Guide](https://github.com/twitter-archive/commons/blob/master/src/java/com/twitter/common/styleguide.md#variable-naming).

---

# Documentation Guidelines

## Annotations

**Override:** Always use `@Override` annotation when overriding.

**Null values:** Use `@Nullable` when a field or method returns null.

## Comments

Try to write comments that explain *why* the code is implemented the way it is, not *what* it 
is doing. If a comment explaining what the code is doing is necessary, consider refactoring the 
code to make it more clear instead.

**TODO and FIXME:** Use `TODO` for things that need to be implemented, and `FIXME` for problems 
that need to be revisited. These are different from regular comments because they are actionable.
Consider leaving a reference to yourself with the comment so other know who to ask about it.

```java
// FIXME (Steve): need to figure this out
// TODO (Steve): need to implement

```

## Javadoc

Use javadoc (`/**`) to document all classes and *public* methods. Should document what the 
class/method *does*. Avoid unnecessary metadata, such as “*An abstract class that...*”. Public 
methods should include tags (with explanations):

- `@param` to explain each parameter,
- `@returns` to explain the return value for non-void methods, and
- `@throws` to explain thrown exceptions.

There should be an empty line between the overall description of the method, and the tags. 
Javadoc (and tags) can be used for private methods as well, but isn’t strictly mandatory.

```java
// Avoid (❌) No tags
/**
* Takes a mathematical expression in infix notation and
* returns the same expression as a list of tokens
* (values and operations).
*/
private List<String> parseExpression(String expression)
        throws IllegalArgumentException {

// Avoid (❌) No blank line before tags
/**
* Returns whether the connection is currently available,
* or in use.
* @return whether the connection is available.
*/
public boolean connectionIsAvailable() {

// Prefer (✔️)
/**
* Takes a mathematical expression in infix notation and
* returns the same expression as a list of tokens
* (values and operations).
*
* @param expression the infix mathematical expression
*                   to parse.
* @return a list of tokens in infix notation order.
* @throws IllegalArgumentException if given expression
*                                  isn't a valid
*                                  mathematical expression.
*/
private List<String> parseExpression(String expression)
        throws IllegalArgumentException {

```

Some insight on how to write meaningful documentation from Twitter’s Java style guide:

```java
// Avoid (❌) - The doc tells nothing that the method
// declaration didn't. This is the 'filler doc'.
// It would pass style checks, but doesn't help anybody.
/**
 * Splits a string.
 *
 * @param s A string.
 * @return A list of strings.
 */
List<String> split(String s);

// Better (✔️) - We know what the method splits on.
// Still some undefined behavior.
/**
 * Splits a string on whitespace.
 *
 * @param s The string to split. An {@code null} string
 *          is treated as an empty string.
 * @return A list of the whitespace-delimited parts
 *         of the input.
 */
List<String> split(String s);

// Great (✔️) - Covers yet another edge case.
/**
 * Splits a string on whitespace. Repeated whitespace
 * characters are collapsed.
 *
 * @param s The string to split. An {@code null} string
 *          is treated as an empty string.
 * @return A list of the whitespace-delimited parts
 *         of the input.
 */
List<String> split(String s);

```

## Unit tests

Use descriptive names for unit tests, as outlined in 
[https://trello.com/c/UhPnKUUZ/5-code-conventions](https://trello.com/c/UhPnKUUZ/5-code-conventions).

Use the annotation `@DisplayName` to describe what is being tested, and what the expected 
outcome is. If it is difficult to clearly state, it is a sign that perhaps the test is too 
complex. Consider refactoring the test, or the code being tested.

```java
@Test
@DisplayName("Adding 1 + 1 returns 2")
public void addingTwoNumbersReturnsCorrectAnswer()

```

However, if a test would benefit from more documentation - such as describing the input, what 
is being tested, and the conditions under which the test passes - then javadoc can be used to do so.

Some examples are taken from 
[A short summary of Java coding best practices](https://rhamedy.medium.com/a-short-summary-of-java-coding-best-practices-31283d0167d3), 
and [Twitter’s Java Style Guide](https://github.com/twitter-archive/commons/blob/master/src/java/com/twitter/common/styleguide.md#variable-naming).

---